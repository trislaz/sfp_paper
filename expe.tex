\section{Experimental Setting}

\subsection{WSI preprocessing}

Preprocessing on a downsampled version of the WSIs was applied to select only tissue area and non-overlapping tiles of 224x224 pixels were extracted at a resolution of 1 mpp. (Details in Supplementary Materials)

\subsection{Data splits for cross-validation}
To measure the performances of our models we performed 3-fold cross-validation for all our training settings. Because the annotated tiles used in our joint-optimization step were directly extracted from the slides themselves, we carefully split the tiles such that tiles in different folds were guaranteed to originate from different slides. The split divided the slides and tiles into a training set, a validation set and a test set. \\ 
All subsequent performance results are then reported as the average and standard deviation of the performance results on each of these 3 test folds.

\subsection{Feature extractor pre-training}
The feature extractor is initialized with pre-trained weights obtained with three distinct supervision policies: fully supervised, self-supervised or a mix of supervision. The three policies rely on the fine-tuning of a DenseNet121 \citep{huang_densely_2016} pre-trained on ImageNet. The fully-supervised architecture consists of an image classification network, fine-tuned with the fully-supervised dataset. The SSL architecture is derived from SimCLR framework and is trained on an unlabeled dataset of 1 million tiles extracted from the slides. Finally for the mixed-supervised architecture, a supervised branch is added to the previous SSL network and trained using the mixed objective function (see Fig. \ref{fig:pipeline_summary} and Eq. \ref{eq:joint-eq}) on the fully supervised dataset. Technical details of these three training settings are available in the supplementary material.


\subsection{Whole Slide Classification}
After tiling the slides, the frozen feature extractor (DenseNet121) was applied to extract meaningful representations from the tiles. This feature extractor was initialized sequentially with the pre-trained weights mentioned above and generated as many sets of features. These bags of features were then used to train the Attention-MIL model with SORS loss applied slide-wise. (Supplementary Materials).

\subsection{Feature Visualization}
To select the most relevant features, we trained an unbiased linear model on the feature vectors extracted from the annotated tiles. The feature vectors were normalized and divided element-wise by the vector of featuresâ€™ standard deviation across all the tiles. The weights of the linear model were used to determine which features were the most impactful for each class. Feature visualizations were generated for the selected features and for each set of pre-trained weights. We extracted the tiles expressing the most of these features by selecting the feature vectors with the higher activation for the concerned feature. Implementation details are provided in Supplementary Materials. 


